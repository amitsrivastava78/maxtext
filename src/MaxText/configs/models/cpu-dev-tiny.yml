# Copyright 2023â€“2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Ultra-tiny model for fast CPU development and testing
# Perfect for cascade attention development on macOS

base_emb_dim: 32          # Very small embedding dimension
base_num_query_heads: 4   # Minimal heads
base_num_kv_heads: 4      # Minimal KV heads
base_mlp_dim: 64          # Small MLP
base_num_decoder_layers: 2  # Just 2 layers for fast iteration
head_dim: 8               # Tiny head dimension
trainable_position_size: 512  # Short sequences
max_target_length: 128    # Very short for CPU
vocab_size: 1024          # Small vocab
per_device_batch_size: 1  # Single batch for CPU

# Model settings
mlp_activations: ["gelu"]
enable_dropout: False
logits_via_embedding: True
normalize_embedding_logits: False
logits_dot_in_fp32: False
normalization_layer_epsilon: 1.e-05
use_iota_embed: True
fused_qkv: True
decoder_block: "llama2"   # Simple Llama2 architecture

# Optimizer (for training if needed)
opt_type: "adam_pax"
learning_rate: 3.e-4
gradient_clipping_threshold: 1.0
adam_b1: 0.9
adam_b2: 0.95
adam_eps: 1.e-8
adam_weight_decay: 0.1

# Attention - use dot_product for CPU compatibility
attention: "dot_product"
